# Skill authoring best practice

Agent Skills

Learn how to write effective Skills that Claude can discover and use successfully.

Good Skills are concise, well-structured, and tested with real usage. This guide provides practical authoring decisions to help you write Skills that Claude can discover and use effectively.

For conceptual background on how Skills work, see the [Skills overview](https://platform.claude.com/docs/en/agents-and-tools/agent-skills/overview).

## 

Core principles

### 

Concise is key

The [context window](https://platform.claude.com/docs/en/build-with-claude/context-windows) is a public good. Your Skill shares the context window with everything else Claude needs to know, including:

-   The system prompt
-   Conversation history
-   Other Skills' metadata
-   Your actual request

Not every token in your Skill has an immediate cost. At startup, only the metadata (name and description) from all Skills is pre-loaded. Claude reads SKILL.md only when the Skill becomes relevant, and reads additional files only as needed. However, being concise in SKILL.md still matters: once Claude loads it, every token competes with conversation history and other context.

**Default assumption**: Claude is already very smart

Only add context Claude doesn't already have. Challenge each piece of information:

-   "Does Claude really need this explanation?"
-   "Can I assume Claude knows this?"
-   "Does this paragraph justify its token cost?"

**Good example: Concise** (approximately 50 tokens):

**Bad example: Too verbose** (approximately 150 tokens):

The concise version assumes Claude knows what PDFs are and how libraries work.

### 

Set appropriate degrees of freedom

Match the level of specificity to the task's fragility and variability.

**High freedom** (text-based instructions):

Use when:

-   Multiple approaches are valid
-   Decisions depend on context
-   Heuristics guide the approach

Example:

**Medium freedom** (pseudocode or scripts with parameters):

Use when:

-   A preferred pattern exists
-   Some variation is acceptable
-   Configuration affects behavior

Example:

**Low freedom** (specific scripts, few or no parameters):

Use when:

-   Operations are fragile and error-prone
-   Consistency is critical
-   A specific sequence must be followed

Example:

**Analogy**: Think of Claude as a robot exploring a path:

-   **Narrow bridge with cliffs on both sides**: There's only one safe way forward. Provide specific guardrails and exact instructions (low freedom). Example: database migrations that must run in exact sequence.
-   **Open field with no hazards**: Many paths lead to success. Give general direction and trust Claude to find the best route (high freedom). Example: code reviews where context determines the best approach.

### 

Test with all models you plan to use

Skills act as additions to models, so effectiveness depends on the underlying model. Test your Skill with all the models you plan to use it with.

**Testing considerations by model**:

-   **Claude Haiku** (fast, economical): Does the Skill provide enough guidance?
-   **Claude Sonnet** (balanced): Is the Skill clear and efficient?
-   **Claude Opus** (powerful reasoning): Does the Skill avoid over-explaining?

What works perfectly for Opus might need more detail for Haiku. If you plan to use your Skill across multiple models, aim for instructions that work well with all of them.

## 

Skill structure

**YAML Frontmatter**: The SKILL.md frontmatter requires two fields:

```
name
```
:

-   Maximum 64 characters
-   Must contain only lowercase letters, numbers, and hyphens
-   Cannot contain XML tags
-   Cannot contain reserved words: "anthropic", "claude"

```
description
```
:

-   Must be non-empty
-   Maximum 1024 characters
-   Cannot contain XML tags
-   Should describe what the Skill does and when to use it

For complete Skill structure details, see the [Skills overview](https://platform.claude.com/docs/en/agents-and-tools/agent-skills/overview#skill-structure).

### 

Naming conventions

Use consistent naming patterns to make Skills easier to reference and discuss. We recommend using **gerund form** (verb + -ing) for Skill names, as this clearly describes the activity or capability the Skill provides.

Remember that the 
```
name
```
 field must use lowercase letters, numbers, and hyphens only.

**Good naming examples (gerund form)**:

-   ```
    processing-pdfs
    ```
    
-   ```
    analyzing-spreadsheets
    ```
    
-   ```
    managing-databases
    ```
    
-   ```
    testing-code
    ```
    
-   ```
    writing-documentation
    ```

**Acceptable alternatives**:

-   Noun phrases: 
    ```
    pdf-processing
    ```
    , 
    ```
    spreadsheet-analysis
    ```
    
-   Action-oriented: 
    ```
    process-pdfs
    ```
    , 
    ```
    analyze-spreadsheets
    ```

**Avoid**:

-   Vague names: 
    ```
    helper
    ```
    , 
    ```
    utils
    ```
    , 
    ```
    tools
    ```
    
-   Overly generic: 
    ```
    documents
    ```
    , 
    ```
    data
    ```
    , 
    ```
    files
    ```
    
-   Reserved words: 
    ```
    anthropic-helper
    ```
    , 
    ```
    claude-tools
    ```
    
-   Inconsistent patterns within your skill collection

Consistent naming makes it easier to:

-   Reference Skills in documentation and conversations
-   Understand what a Skill does at a glance
-   Organize and search through multiple Skills
-   Maintain a professional, cohesive skill library

### 

Writing effective descriptions

The 
```
description
```
 field enables Skill discovery and should include both what the Skill does and when to use it.

**Always write in third person**. The description is injected into the system prompt, and inconsistent point-of-view can cause discovery problems.

-   **Good:** "Processes Excel files and generates reports"
-   **Avoid:** "I can help you process Excel files"
-   **Avoid:** "You can use this to process Excel files"

**Be specific and include key terms**. Include both what the Skill does and specific triggers/contexts for when to use it.

Each Skill has exactly one description field. The description is critical for skill selection: Claude uses it to choose the right Skill from potentially 100+ available Skills. Your description must provide enough detail for Claude to know when to select this Skill, while the rest of SKILL.md provides the implementation details.

Effective examples:

**PDF Processing skill:**

**Excel Analysis skill:**

**Git Commit Helper skill:**

Avoid vague descriptions like these:

### 

Progressive disclosure patterns

SKILL.md serves as an overview that points Claude to detailed materials as needed, like a table of contents in an onboarding guide. For an explanation of how progressive disclosure works, see [How Skills work](https://platform.claude.com/docs/en/agents-and-tools/agent-skills/overview#how-skills-work) in the overview.

**Practical guidance:**

-   Keep SKILL.md body under 500 lines for optimal performance
-   Split content into separate files when approaching this limit
-   Use the patterns below to organize instructions, code, and resources effectively

#### 

Visual overview: From simple to complex

A basic Skill starts with just a SKILL.md file containing metadata and instructions:

![Simple SKILL.md file showing YAML frontmatter and markdown body](https://platform.claude.com/docs/images/agent-skills-simple-file.png)

As your Skill grows, you can bundle additional content that Claude loads only when needed:

![Bundling additional reference files like reference.md and forms.md.](https://platform.claude.com/docs/images/agent-skills-bundling-content.png)

The complete Skill directory structure might look like this:

#### 

Pattern 1: High-level guide with references

Claude loads FORMS.md, REFERENCE.md, or EXAMPLES.md only when needed.

#### 

Pattern 2: Domain-specific organization

For Skills with multiple domains, organize content by domain to avoid loading irrelevant context. When a user asks about sales metrics, Claude only needs to read sales-related schemas, not finance or marketing data. This keeps token usage low and context focused.

#### 

Pattern 3: Conditional details

Show basic content, link to advanced content:

Claude reads REDLINING.md or OOXML.md only when the user needs those features.

### 

Avoid deeply nested references

Claude may partially read files when they're referenced from other referenced files. When encountering nested references, Claude might use commands like 
```
head -100
```
 to preview content rather than reading entire files, resulting in incomplete information.

**Keep references one level deep from SKILL.md**. All reference files should link directly from SKILL.md to ensure Claude reads complete files when needed.

**Bad example: Too deep**:

**Good example: One level deep**:

### 

Structure longer reference files with table of contents

For reference files longer than 100 lines, include a table of contents at the top. This ensures Claude can see the full scope of available information even when previewing with partial reads.

**Example**:

Claude can then read the complete file or jump to specific sections as needed.

For details on how this filesystem-based architecture enables progressive disclosure, see the [Runtime environment](#runtime-environment) section in the Advanced section below.

## 

Workflows and feedback loops

### 

Use workflows for complex tasks

Break complex operations into clear, sequential steps. For particularly complex workflows, provide a checklist that Claude can copy into its response and check off as it progresses.

**Example 1: Research synthesis workflow** (for Skills without code):

This example shows how workflows apply to analysis tasks that don't require code. The checklist pattern works for any complex, multi-step process.

**Example 2: PDF form filling workflow** (for Skills with code):

Clear steps prevent Claude from skipping critical validation. The checklist helps both Claude and you track progress through multi-step workflows.

### 

Implement feedback loops

**Common pattern**: Run validator → fix errors → repeat

This pattern greatly improves output quality.

**Example 1: Style guide compliance** (for Skills without code):

This shows the validation loop pattern using reference documents instead of scripts. The "validator" is STYLE\_GUIDE.md, and Claude performs the check by reading and comparing.

**Example 2: Document editing process** (for Skills with code):

The validation loop catches errors early.

## 

Content guidelines

### 

Avoid time-sensitive information

Don't include information that will become outdated:

**Bad example: Time-sensitive** (will become wrong):

**Good example** (use "old patterns" section):

The old patterns section provides historical context without cluttering the main content.

### 

Use consistent terminology

Choose one term and use it throughout the Skill:

**Good - Consistent**:

-   Always "API endpoint"
-   Always "field"
-   Always "extract"

**Bad - Inconsistent**:

-   Mix "API endpoint", "URL", "API route", "path"
-   Mix "field", "box", "element", "control"
-   Mix "extract", "pull", "get", "retrieve"

Consistency helps Claude understand and follow instructions.

## 

Common patterns

### 

Template pattern

Provide templates for output format. Match the level of strictness to your needs.

**For strict requirements** (like API responses or data formats):

**For flexible guidance** (when adaptation is useful):

### 

Examples pattern

For Skills where output quality depends on seeing examples, provide input/output pairs just like in regular prompting:

Examples help Claude understand the desired style and level of detail more clearly than descriptions alone.

### 

Conditional workflow pattern

Guide Claude through decision points:

If workflows become large or complicated with many steps, consider pushing them into separate files and tell Claude to read the appropriate file based on the task at hand.

## 

Evaluation and iteration

### 

Build evaluations first

**Create evaluations BEFORE writing extensive documentation.** This ensures your Skill solves real problems rather than documenting imagined ones.

**Evaluation-driven development:**

1.  **Identify gaps**: Run Claude on representative tasks without a Skill. Document specific failures or missing context
2.  **Create evaluations**: Build three scenarios that test these gaps
3.  **Establish baseline**: Measure Claude's performance without the Skill
4.  **Write minimal instructions**: Create just enough content to address the gaps and pass evaluations
5.  **Iterate**: Execute evaluations, compare against baseline, and refine

This approach ensures you're solving actual problems rather than anticipating requirements that may never materialize.

**Evaluation structure**:

This example demonstrates a data-driven evaluation with a simple testing rubric. We do not currently provide a built-in way to run these evaluations. Users can create their own evaluation system. Evaluations are your source of truth for measuring Skill effectiveness.

### 

Develop Skills iteratively with Claude

The most effective Skill development process involves Claude itself. Work with one instance of Claude ("Claude A") to create a Skill that will be used by other instances ("Claude B"). Claude A helps you design and refine instructions, while Claude B tests them in real tasks. This works because Claude models understand both how to write effective agent instructions and what information agents need.

**Creating a new Skill:**

1.  **Complete a task without a Skill**: Work through a problem with Claude A using normal prompting. As you work, you'll naturally provide context, explain preferences, and share procedural knowledge. Notice what information you repeatedly provide.
    
2.  **Identify the reusable pattern**: After completing the task, identify what context you provided that would be useful for similar future tasks.
    
    5.*Example**: If you worked through a BigQuery analysis, you might have provided table names, field definitions, filtering rules (like "always exclude test accounts"), and common query patterns.
    
3.  **Ask Claude A to create a Skill**: "Create a Skill that captures this BigQuery analysis pattern we just used. Include the table schemas, naming conventions, and the rule about filtering test accounts."
    
    Claude models understand the Skill format and structure natively. You don't need special system prompts or a "writing skills" skill to get Claude to help create Skills. Simply ask Claude to create a Skill and it will generate properly structured SKILL.md content with appropriate frontmatter and body content.
    
4.  **Review for conciseness**: Check that Claude A hasn't added unnecessary explanations. Ask: "Remove the explanation about what win rate means - Claude already knows that."
    
5.  **Improve information architecture**: Ask Claude A to organize the content more effectively. For example: "Organize this so the table schema is in a separate reference file. We might add more tables later."
    
6.  **Test on similar tasks**: Use the Skill with Claude B (a fresh instance with the Skill loaded) on related use cases. Observe whether Claude B finds the right information, applies rules correctly, and handles the task successfully.
    
7.  **Iterate based on observation**: If Claude B struggles or misses something, return to Claude A with specifics: "When Claude used this Skill, it forgot to filter by date for Q4. Should we add a section about date filtering patterns?"

**Iterating on existing Skills:**

The same hierarchical pattern continues when improving Skills. You alternate between:

-   **Working with Claude A** (the expert who helps refine the Skill)
-   **Testing with Claude B** (the agent using the Skill to perform real work)
-   **Observing Claude B's behavior** and bringing insights back to Claude A
1.  **Use the Skill in real workflows**: Give Claude B (with the Skill loaded) actual tasks, not test scenarios
    
2.  **Observe Claude B's behavior**: Note where it struggles, succeeds, or makes unexpected choices
    
    5.*Example observation**: "When I asked Claude B for a regional sales report, it wrote the query but forgot to filter out test accounts, even though the Skill mentions this rule."
    
3.  **Return to Claude A for improvements**: Share the current SKILL.md and describe what you observed. Ask: "I noticed Claude B forgot to filter test accounts when I asked for a regional report. The Skill mentions filtering, but maybe it's not prominent enough?"
    
4.  **Review Claude A's suggestions**: Claude A might suggest reorganizing to make rules more prominent, using stronger language like "MUST filter" instead of "always filter", or restructuring the workflow section.
    
5.  **Apply and test changes**: Update the Skill with Claude A's refinements, then test again with Claude B on similar requests
    
6.  **Repeat based on usage**: Continue this observe-refine-test cycle as you encounter new scenarios. Each iteration improves the Skill based on real agent behavior, not assumptions.

**Gathering team feedback:**

1.  Share Skills with teammates and observe their usage
2.  Ask: Does the Skill activate when expected? Are instructions clear? What's missing?
3.  Incorporate feedback to address blind spots in your own usage patterns

**Why this approach works**: Claude A understands agent needs, you provide domain expertise, Claude B reveals gaps through real usage, and iterative refinement improves Skills based on observed behavior rather than assumptions.

### 

Observe how Claude navigates Skills

As you iterate on Skills, pay attention to how Claude actually uses them in practice. Watch for:

-   **Unexpected exploration paths**: Does Claude read files in an order you didn't anticipate? This might indicate your structure isn't as intuitive as you thought
-   **Missed connections**: Does Claude fail to follow references to important files? Your links might need to be more explicit or prominent
-   **Overreliance on certain sections**: If Claude repeatedly reads the same file, consider whether that content should be in the main SKILL.md instead
-   **Ignored content**: If Claude never accesses a bundled file, it might be unnecessary or poorly signaled in the main instructions

Iterate based on these observations rather than assumptions. The 'name' and 'description' in your Skill's metadata are particularly critical. Claude uses these when deciding whether to trigger the Skill in response to the current task. Make sure they clearly describe what the Skill does and when it should be used.

## 

Anti-patterns to avoid

### 

Avoid Windows-style paths

Always use forward slashes in file paths, even on Windows:

-   ✓ **Good**: 
    ```
    scripts/helper.py
    ```
    , 
    ```
    reference/guide.md
    ```
    
-   ✗ **Avoid**: 
    ```
    scripts\helper.py
    ```
    , 
    ```
    reference\guide.md
    ```

Unix-style paths work across all platforms, while Windows-style paths cause errors on Unix systems.

### 

Avoid offering too many options

Don't present multiple approaches unless necessary:

## 

Advanced: Skills with executable code

The sections below focus on Skills that include executable scripts. If your Skill uses only markdown instructions, skip to [Checklist for effective Skills](#checklist-for-effective-skills).

### 

Solve, don't punt

When writing scripts for Skills, handle error conditions rather than punting to Claude.

**Good example: Handle errors explicitly**:

**Bad example: Punt to Claude**:

Configuration parameters should also be justified and documented to avoid "voodoo constants" (Ousterhout's law). If you don't know the right value, how will Claude determine it?

**Good example: Self-documenting**:

**Bad example: Magic numbers**:

### 

Provide utility scripts

Even if Claude could write a script, pre-made scripts offer advantages:

**Benefits of utility scripts**:

-   More reliable than generated code
-   Save tokens (no need to include code in context)
-   Save time (no code generation required)
-   Ensure consistency across uses

![Bundling executable scripts alongside instruction files](https://platform.claude.com/docs/images/agent-skills-executable-scripts.png)

The diagram above shows how executable scripts work alongside instruction files. The instruction file (forms.md) references the script, and Claude can execute it without loading its contents into context.

**Important distinction**: Make clear in your instructions whether Claude should:

-   **Execute the script** (most common): "Run 
    ```
    analyze_form.py
    ```
     to extract fields"
-   **Read it as reference** (for complex logic): "See 
    ```
    analyze_form.py
    ```
     for the field extraction algorithm"

For most utility scripts, execution is preferred because it's more reliable and efficient. See the [Runtime environment](#runtime-environment) section below for details on how script execution works.

**Example**:

### 

Use visual analysis

When inputs can be rendered as images, have Claude analyze them:

In this example, you'd need to write the 
```
pdf_to_images.py
```
 script.

Claude's vision capabilities help understand layouts and structures.

When Claude performs complex, open-ended tasks, it can make mistakes. The "plan-validate-execute" pattern catches errors early by having Claude first create a plan in a structured format, then validate that plan with a script before executing it.

**Example**: Imagine asking Claude to update 50 form fields in a PDF based on a spreadsheet. Without validation, Claude might reference non-existent fields, create conflicting values, miss required fields, or apply updates incorrectly.

**Solution**: Use the workflow pattern shown above (PDF form filling), but add an intermediate 
```
changes.json
```
 file that gets validated before applying changes. The workflow becomes: analyze → **create plan file** → **validate plan** → execute → verify.

**Why this pattern works:**

-   **Catches errors early**: Validation finds problems before changes are applied
-   **Machine-verifiable**: Scripts provide objective verification
-   **Reversible planning**: Claude can iterate on the plan without touching originals
-   **Clear debugging**: Error messages point to specific problems

**When to use**: Batch operations, destructive changes, complex validation rules, high-stakes operations.

**Implementation tip**: Make validation scripts verbose with specific error messages like "Field 'signature\_date' not found. Available fields: customer\_name, order\_total, signature\_date\_signed" to help Claude fix issues.

### 

Package dependencies

Skills run in the code execution environment with platform-specific limitations:

-   **claude.ai**: Can install packages from npm and PyPI and pull from GitHub repositories
-   **Anthropic API**: Has no network access and no runtime package installation

List required packages in your SKILL.md and verify they're available in the [code execution tool documentation](https://platform.claude.com/docs/en/agents-and-tools/tool-use/code-execution-tool).

### 

Runtime environment

Skills run in a code execution environment with filesystem access, bash commands, and code execution capabilities. For the conceptual explanation of this architecture, see [The Skills architecture](https://platform.claude.com/docs/en/agents-and-tools/agent-skills/overview#the-skills-architecture) in the overview.

**How this affects your authoring:**

**How Claude accesses Skills:**

1.  **Metadata pre-loaded**: At startup, the name and description from all Skills' YAML frontmatter are loaded into the system prompt
2.  **Files read on-demand**: Claude uses bash Read tools to access SKILL.md and other files from the filesystem when needed
3.  **Scripts executed efficiently**: Utility scripts can be executed via bash without loading their full contents into context. Only the script's output consumes tokens
4.  **No context penalty for large files**: Reference files, data, or documentation don't consume context tokens until actually read
-   **File paths matter**: Claude navigates your skill directory like a filesystem. Use forward slashes (
    ```
    reference/guide.md
    ```
    ), not backslashes
-   **Name files descriptively**: Use names that indicate content: 
    ```
    form_validation_rules.md
    ```
    , not 
    ```
    doc2.md
    ```
    
-   **Organize for discovery**: Structure directories by domain or feature  -   Good: 
          ```
          reference/finance.md
          ```
          , 
          ```
          reference/sales.md
          ```
          
      -   Bad: 
          ```
          docs/file1.md
          ```
          , 
          ```
          docs/file2.md
          ```
    
-   **Bundle comprehensive resources**: Include complete API docs, extensive examples, large datasets; no context penalty until accessed
-   **Prefer scripts for deterministic operations**: Write 
    ```
    validate_form.py
    ```
     rather than asking Claude to generate validation code
-   **Make execution intent clear**:  -   "Run 
          ```
          analyze_form.py
          ```
           to extract fields" (execute)
      -   "See 
          ```
          analyze_form.py
          ```
           for the extraction algorithm" (read as reference)
    
-   **Test file access patterns**: Verify Claude can navigate your directory structure by testing with real requests

**Example:**

When the user asks about revenue, Claude reads SKILL.md, sees the reference to 
```
reference/finance.md
```
, and invokes bash to read just that file. The sales.md and product.md files remain on the filesystem, consuming zero context tokens until needed. This filesystem-based model is what enables progressive disclosure. Claude can navigate and selectively load exactly what each task requires.

For complete details on the technical architecture, see [How Skills work](https://platform.claude.com/docs/en/agents-and-tools/agent-skills/overview#how-skills-work) in the Skills overview.

### 

MCP tool references

If your Skill uses MCP (Model Context Protocol) tools, always use fully qualified tool names to avoid "tool not found" errors.

**Format**: 
```
ServerName:tool_name
```

**Example**:

Where:

-   ```
    BigQuery
    ```
     and 
    ```
    GitHub
    ```
     are MCP server names
-   ```
    bigquery_schema
    ```
     and 
    ```
    create_issue
    ```
     are the tool names within those servers

Without the server prefix, Claude may fail to locate the tool, especially when multiple MCP servers are available.

### 

Avoid assuming tools are installed

Don't assume packages are available:

## 

Technical notes

### 

YAML frontmatter requirements

The SKILL.md frontmatter requires 
```
name
```
 and 
```
description
```
 fields with specific validation rules:

-   ```
    name
    ```
    : Maximum 64 characters, lowercase letters/numbers/hyphens only, no XML tags, no reserved words
-   ```
    description
    ```
    : Maximum 1024 characters, non-empty, no XML tags

See the [Skills overview](https://platform.claude.com/docs/en/agents-and-tools/agent-skills/overview#skill-structure) for complete structure details.

### 

Token budgets

Keep SKILL.md body under 500 lines for optimal performance. If your content exceeds this, split it into separate files using the progressive disclosure patterns described earlier. For architectural details, see the [Skills overview](https://platform.claude.com/docs/en/agents-and-tools/agent-skills/overview#how-skills-work).

## 

Checklist for effective Skills

Before sharing a Skill, verify:

### 

Core quality

-   Description is specific and includes key terms
-   Description includes both what the Skill does and when to use it
-   SKILL.md body is under 500 lines
-   Additional details are in separate files (if needed)
-   No time-sensitive information (or in "old patterns" section)
-   Consistent terminology throughout
-   Examples are concrete, not abstract
-   File references are one level deep
-   Progressive disclosure used appropriately
-   Workflows have clear steps

### 

Code and scripts

-   Scripts solve problems rather than punt to Claude
-   Error handling is explicit and helpful
-   No "voodoo constants" (all values justified)
-   Required packages listed in instructions and verified as available
-   Scripts have clear documentation
-   No Windows-style paths (all forward slashes)
-   Validation/verification steps for critical operations
-   Feedback loops included for quality-critical tasks

### 

Testing

-   At least three evaluations created
-   Tested with Haiku, Sonnet, and Opus
-   Tested with real usage scenarios
-   Team feedback incorporated (if applicable)

## 

Next steps